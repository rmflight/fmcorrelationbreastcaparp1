# Data Munging

## How many lines of data do we have?

```{r count_lines}
use_dir <- "/mlab/data/rmflight/Documents/projects/work/fondufe-mittendorf_lab/parp1_data/"
ln4_files <- dir(use_dir, pattern = "YFM_LN4")
ln4_lines <- sapply(ln4_files, function(x){
  file_loc <- file.path(use_dir, x)
  R.utils::countLines(file_loc)
})
sum(ln4_lines)

ln5_files <- dir(use_dir, pattern = "YFM_LN5")
ln5_lines <- sapply(ln5_files, function(x){
  file_loc <- file.path(use_dir, x)
  R.utils::countLines(file_loc)
})
sum(ln5_lines)
```


## Data Check

Based on the data we have, duplicate reads were removed. So we should have a maximum count value of 6 (if we ignore strands) at any given base position. This is based on having 3 replicate experiments, and the positive and negative stranded data (3 x 2). This should be verified for at least the largest chromosome.

```{r checkChrom3}
data_dir <- "/mlab/data/rmflight/Documents/projects/work/fondufe-mittendorf_lab/parp1_data"
library(GenomicRanges)
chr3_file <- file.path(data_dir, "YFM_LN4_chr3.csv")
chr3_data <- read.table(chr3_file, header = TRUE, sep = ",")
chr3_ranges <- GRanges(seqnames = "chr3",
                        ranges = IRanges(start = chr3_data$startx, width = 1),
                        strand = chr3_data$strand)
rm(chr3_data)
strand(chr3_ranges) <- "*"
```

```{r make_unique}
chr3_unique <- unique(chr3_ranges)
length(chr3_ranges)
length(chr3_unique)
```

Now lets count how many are at each location.

```{r count_overlap}
chr3_overlap <- countOverlaps(chr3_unique, chr3_ranges)
mcols(chr3_unique)$overlap <- chr3_overlap
chr3_unique <- sort(chr3_unique)
```

```{r how_many}
chr3_overlap_rle <- rle(sort(chr3_overlap))
chr3_counts <- chr3_overlap_rle$values
names(chr3_counts) <- chr3_overlap_rle$lengths
chr3_counts
```

Wow. Some of these have really large counts. One might expect to get counts >= 20 or 30, but having counts of > 100 points to some intrinsic bias. Given that all of the bead id's are unique, indicating that there are unique reads, then there is something odd about these regions. We will double check how many exist across the chromosomes, and whether they intersect other regions of interest, such as TSS. 

Here is a question: what is the maximum value that corresponds to working with 99% of the reads? Could we use this as a maximum cutoff value?

```{r get_99}
chr3_totals <- data.frame(n_loc = chr3_overlap_rle$lengths, count_at_loc = chr3_overlap_rle$values)
chr3_totals$tot_reads <- chr3_totals$n_loc * chr3_totals$count_at_loc
chr3_totals$cum_reads <- cumsum(chr3_totals$tot_reads)
chr3_totals$perc_reads <- chr3_totals$cum_reads / sum(chr3_totals$tot_reads) * 100
```

From this, it looks like we should have a maximum at **5**. This is in line with what we expect based on the data available. We should do this across all of the chromosomes, however.

```{r get_all_counts}
use_dir <- "/mlab/data/rmflight/Documents/projects/work/fondufe-mittendorf_lab/parp1_data/"
library(parallel)
library(parp1)
library(magrittr)
library(GenomicRanges)
options(mc.cores = 10)
ln4_reads <- mclapply(file.path(use_dir, ln4_files), function(in_file){
  chr_data <- read.table(in_file, header = TRUE, sep = ",")
  chr_ranges <- GRanges(seqnames = get_chr(in_file),
                        ranges = IRanges(start = chr_data$startx, width = 1),
                        strand = chr_data$strand)
  rm(chr_data)
  strand(chr_ranges) <- "*"
  chr_unique <- unique(chr_ranges)
  chr_overlap <- countOverlaps(chr_unique, chr_ranges)
  chr_overlap_rle <- rle(sort(chr_overlap))
  chr_overlap_rle
})
```

```{r compile_counts}
all_values <- lapply(ln4_reads, function(x){x$values}) %>% do.call(c, .) %>% unique() %>% sort()

all_counts <- data.frame(values = all_values, counts = 0)
rownames(all_counts) <- all_values
for (i_chr in seq(1, length(ln4_reads))){
  tmp_rle <- ln4_reads[[i_chr]]
  use_indx <- as.character(tmp_rle$values)
  all_counts[use_indx, "counts"] <- all_counts[use_indx, "counts"] + tmp_rle$lengths
}

all_counts$tot_reads <- all_counts$values * all_counts$counts
all_counts <- all_counts[order(all_counts$tot_reads, decreasing = TRUE),]
all_counts$cum_reads <- cumsum(all_counts$tot_reads)
all_counts$perc_reads <- all_counts$cum_reads / max(all_counts$cum_reads) * 100
head(all_counts)
```

And there we have it! At 6 reads, we are accounting for 99% of the reads available. The other reads make up less than 1% of the data. 6 is also the ideal maximum expected based on the data we have.

So now, for each chromosome, we will go through and generate a `GRanges` object with unique locations and the number of counts at each position, capping the counts at **6** for any given position.

```{r ln4_capped_granges, eval = FALSE}
ln4_files_process <- file.path(use_dir, ln4_files)

ln4_reads <- GRanges(seqnames = "chr1", ranges = IRanges(start = 1, end = 1), strand = "*")

for (i_file in ln4_files_process){
	tmp_reads <- read.table(i_file, sep = ",", header = TRUE)
	use_chr <- get_chr(i_file, "_")
	ln4_reads <- c(ln4_reads, GRanges(seqnames = use_chr,
					  ranges = IRanges(start = tmp_reads[, "startx"], width = 1),
					  strand = "*"))
}
ln4_reads <- ln4_reads[2:(length(ln4_reads))]
save(ln4_reads, file = file.path(use_dir, "ln4_reads_all.RData"))

ln4_unique <- unique(ln4_reads)
ln4_counts <- countOverlaps(ln4_unique, ln4_reads)
ln4_counts[(ln4_counts > 6)] <- 6
mcols(ln4_unique)$n_count <- ln4_counts

save(ln4_unique, file = file.path(use_dir, "ln4_reads_unique.RData"))
```

```{r ln5_capped_granges, eval = FALSE}
ln5_files_process <- file.path(use_dir, dir(use_dir, pattern = "YFM_LN5"))

ln5_reads <- GRanges(seqnames = "chr1", ranges = IRanges(start = 1, end = 1), strand = "*")

for (i_file in ln5_files_process){
	tmp_reads <- read.table(i_file, sep = ",", header = TRUE)
	use_chr <- get_chr(i_file, "_")
	ln5_reads <- c(ln5_reads, GRanges(seqnames = use_chr,
		       ranges = IRanges(start = tmp_reads[, "startx"], width = 1),
		       strand = "*"))
}
ln5_reads <- ln5_reads[2:(length(ln5_reads))]
save(ln5_reads, file = file.path(use_dir, "ln5_reads_all.RData"))

ln5_unique <- unique(ln5_reads)
ln5_counts <- countOverlaps(ln5_unique, ln5_reads)
ln5_counts[(ln5_counts > 6)] <- 6
mcols(ln5_unique)$n_count <- ln5_counts

save(ln5_unique, file = file.path(use_dir, "ln5_reads_unique.RData"))
```

Finally, we also want to know how many reads we have in each case.

```{r count_reads, eval = FALSE}
ln4_counts <- mclapply(ln4_outfiles, function(x){
  load(file.path(use_dir, x))
  sum(mcols(unique_locs)[, "count"])
})
sum(unlist(ln4_counts))

ln5_counts <- mclapply(ln5_outfiles, function(x){
  load(file.path(use_dir, x))
  sum(mcols(unique_locs)[, "count"])
})
sum(unlist(ln5_counts))
```

## TSS Regions

To get association between transcription start site (TSS) regions and other features, we should have the TSS's and their windows already saved in a file.

```{r get_tss_windows, eval = FALSE}
tss_file <- file.path(use_dir, "ensGene_TTSS.csv")
tss_data <- read.table(tss_file, header = TRUE, sep = ",", stringsAsFactors = FALSE)
tss_regions <- GRanges(seqnames = tss_data$chrom,
                       strand = tss_data$strand,
                       ranges = IRanges(start = tss_data$txStart,
                                        end = tss_data$txEnd),
                       names = tss_data$name)
rm(tss_data)

tss_windows <- GRanges(seqnames = seqnames(tss_regions),
                       strand = strand(tss_regions),
                       ranges = IRanges(start = start(tss_regions) - 1000,
                                        end = start(tss_regions) + 1000))
names(tss_windows) <- mcols(tss_regions)$names
save(tss_windows, file = file.path(use_dir, "tss_windows.RData"))
```


## TSS with Parp1

In addition, we can generate the counts of Parp1 nucleosomes with the TSS's and save them.

```{r parp1_overlap_tss, eval=FALSE}
ln4_files <- file.path(use_dir, dir(use_dir, pattern = "ln4_chr"))

ln4_overlap <- get_overlap_counts(ln4_files, tss_windows)

ln5_files <- file.path(use_dir, dir(use_dir, pattern = "ln5_chr"))
ln5_overlap <- get_overlap_counts(ln5_files, tss_windows)
```

```{r parp1_dataframe, eval=FALSE}
total_reads <- c(ln4 = 60228591, ln5 = 53669637)
n_tss <- length(tss_windows)
overlap_values <- DataFrame(matrix(0, nrow = n_tss, ncol = 4), row.names = names(tss_windows))

colnames(overlap_values) <- c("ln4", "ln5", "ln4_frac", "ln5_frac")

overlap_values[names(ln5_overlap), "ln5"] <- ln5_overlap
overlap_values[names(ln4_overlap), "ln4"] <- ln4_overlap
overlap_values[, "ln4_frac"] <- overlap_values[, "ln4"] / total_reads["ln4"]
overlap_values[, "ln5_frac"] <- overlap_values[, "ln5"] / total_reads["ln5"]
```

```{r add_to_tss, eval=FALSE}
mcols(tss_windows) <- overlap_values
save(tss_windows, file = file.path(use_dir, "tss_parp1.RData"))
```


## Other Analyses

In addition to the histone marks analysis that pulled histone peaks directly from the database, we also need to do

* Parp1 correlation with CTCF ChIP-Seq (http://genome.ucsc.edu/cgi-bin/hgFileUi?db=hg19&g=wgEncodeUwTfbs, select peaks from MCF-7 and CTCF)
* Parp1 correlation with expression (http://biogps.org/dataset/844/expression-data-from-breast-cancer-cell-lines-mcf-7-and-mda-mb-231/)
* Parp1 correlation with methylation either from UCSC ENCODE (rep1: http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeHaibMethylRrbs/wgEncodeHaibMethylRrbsMcf7DukeSitesRep1.bed.gz,
rep2: http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeHaibMethylRrbs/wgEncodeHaibMethylRrbsMcf7DukeSitesRep2.bed.gz) or mdanb231_mr1 and mr10 data that already exists, but still need to find a source.