# Data Munging

## How many lines of data do we have?

```{r count_lines}
ln4_files <- dir("../parp1_data", pattern = "YFM_LN4")
ln4_lines <- sapply(ln4_files, function(x){
  file_loc <- file.path("../parp1_data", x)
  R.utils::countLines(file_loc)
})
sum(ln4_lines)


ln5_files <- dir("../parp1_data", patter = "YFM_LN5")
ln5_lines <- sapply(ln5_files, function(x){
  file_loc <- file.path("../parp1_data", x)
  R.utils::countLines(file_loc)
})
sum(ln5_lines)
```


## Data Check

Based on the data we have, duplicate reads were removed. So we should have a maximum count value of 6 (if we ignore strands) at any given base position. This is based on having 3 replicate experiments, and the positive and negative stranded data (3 x 2). This should be verified for at least the largest chromosome.

```{r checkChrom3}
data_dir <- "/mlab/data/rmflight/Documents/projects/work/fondufe-mittendorf_lab/parp1_data"
library(GenomicRanges)
chr3_file <- file.path(data_dir, "YFM_LN4_chr3.csv")
chr3_data <- read.table(chr3_file, header = TRUE, sep = ",")
chr3_ranges <- GRanges(seqnames = "chr3",
                        ranges = IRanges(start = chr3_data$startx, width = 1),
                        strand = chr3_data$strand)
rm(chr3_data)
strand(chr3_ranges) <- "*"
```

```{r make_unique}
chr3_unique <- unique(chr3_ranges)
length(chr3_ranges)
length(chr3_unique)
```

Now lets count how many are at each location.

```{r count_overlap}
chr3_overlap <- countOverlaps(chr3_unique, chr3_ranges)
mcols(chr3_unique)$overlap <- chr3_overlap
chr3_unique <- sort(chr3_unique)
```

```{r how_many}
chr3_overlap_rle <- rle(sort(chr3_overlap))
chr3_counts <- chr3_overlap_rle$values
names(chr3_counts) <- chr3_overlap_rle$lengths
chr3_counts
```

Wow. Some of these have really large counts. One might expect to get counts >= 20 or 30, but having counts of > 100 points to some intrinsic bias. Given that all of the bead id's are unique, indicating that there are unique reads, then there is something odd about these regions. We will double check how many exist across the chromosomes, and whether they intersect other regions of interest, such as TSS. 

Here is a question: what is the maximum value that corresponds to working with 99% of the reads? Could we use this as a maximum cutoff value?

```{r get_99}
chr3_totals <- data.frame(n_loc = chr3_overlap_rle$lengths, count_at_loc = chr3_overlap_rle$values)
chr3_totals$tot_reads <- chr3_totals$n_loc * chr3_totals$count_at_loc
chr3_totals$cum_reads <- cumsum(chr3_totals$tot_reads)
chr3_totals$perc_reads <- chr3_totals$cum_reads / sum(chr3_totals$tot_reads) * 100
```

From this, it looks like we should have a maximum at **5**. This is in line with what we expect based on the data available. We should do this across all of the chromosomes, however.

```{r get_all_counts}
use_dir <- "/mlab/data/rmflight/Documents/projects/work/fondufe-mittendorf_lab/parp1_data/"
library(parallel)
library(parp1)
library(magrittr)
options(mc.cores = 10)
ln4_reads <- mclapply(file.path(use_dir, ln4_files), function(in_file){
  chr_data <- read.table(in_file, header = TRUE, sep = ",")
  chr_ranges <- GRanges(seqnames = get_chr(in_file),
                        ranges = IRanges(start = chr_data$startx, width = 1),
                        strand = chr_data$strand)
  rm(chr_data)
  strand(chr_ranges) <- "*"
  chr_unique <- unique(chr_ranges)
  chr_overlap <- countOverlaps(chr_unique, chr_ranges)
  chr_overlap_rle <- rle(sort(chr_overlap))
  chr_overlap_rle
})
```

```{r compile_counts}
all_values <- lapply(ln4_reads, function(x){x$values}) %>% do.call(c, .) %>% unique() %>% sort()

all_counts <- data.frame(values = all_values, counts = 0)
rownames(all_counts) <- all_values
for (i_chr in seq(1, length(ln4_reads))){
  tmp_rle <- ln4_reads[[i_chr]]
  use_indx <- as.character(tmp_rle$values)
  all_counts[use_indx, "counts"] <- all_counts[use_indx, "counts"] + tmp_rle$lengths
}

all_counts$tot_reads <- all_counts$values * all_counts$counts
all_counts <- all_counts[order(all_counts$tot_reads, decreasing = TRUE),]
all_counts$cum_reads <- cumsum(all_counts$tot_reads)
all_counts$perc_reads <- all_counts$cum_reads / max(all_counts$cum_reads) * 100
head(all_counts)
```

And there we have it! At 6 reads, we are accounting for 99% of the reads available. The other reads make up less than 1% of the data. 6 is also the ideal maximum expected based on the data we have.

So now, for each chromosome, we will go through and generate a `GRanges` object with unique locations and the number of counts at each position, capping the counts at **6** for any given position.

```{r ln4_capped_granges}
ln4_files_process <- file.path(use_dir, ln4_files)

```

